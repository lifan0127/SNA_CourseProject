{
    "contents" : "---\ntitle: \"Milestone Report\"\noutput: html_document\n---\n\nThis milestone report provides a summary of the data processing and descriptive statistics steps for the Capstone project. In addition, strategies about modeling and predictions will also be discussed.\n\n```{r preparation, message=FALSE, warning=FALSE}\nlibrary(tm)\nlibrary(RWeka)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(dplyr)  # version 0.3 required\n\n```\n\n### Data Loading & Preprocessing\n\nFollowing instructions in [Task 0](https://class.coursera.org/dsscapstone-002/wiki/Task_0), the Capstone dataset was downloaded from Coursera website. The raw data is a zip file named \"Coursera-SwiftKey.zip\". After unzipping the file, a collection of text files were obtained. The files contains text materials in English, German, French and Russian. For this project, our focus is on the English documents only. \n\nIn this milestone report, we will use the \"en_US.news.txt\" file as an example and randomly choose 5% data to demonstrate basic data preprocessing and statistical analysis. The full dataset will be used in the final report.\n\n\n```{r load_data}\nf <- file(\"../../DSCapstoneProject_Data/Data/final/en_US/en_US.news.txt\", \"rb\")\nen_us.news <- readLines(f)\nclose(f)\n\nset.seed(123)\nnews <- sample(en_us.news, length(en_us.news)*0.05)\n\nrecord.length <- data.frame(character=nchar(news), word=sapply(strsplit(news, \" \"), length))\n```\n\nThe basic procedure for data preprocessing was adapted from \"Hands-On Data Science with R - Text Mining\" by Graham Williams.^[\\tiny http://handsondatascience.com/TextMiningO.pdf (accessed on 11/16/2014)] It containes the following main steps:\n\n1. Construct corpus (a collection of texts) from the input file.\n2. Clean-up the corpus by removing special characters, punctuation, numbers etc.\n3. Build Document Term Matrix (term frequency table) using single word and n-grams.\n\nOne major issue, as discussed extensively in the forum, is the performance of the tm package, which makes any exploratory activity painfully long if not impossible. we plan to resort to the Microsoft Azure Machine Learning platform which is currently available for free.^[\\tiny http://azure.microsoft.com/en-us/services/machine-learning (accessed on 11/16/2014)]\n\n\n```{r data_preprocessing}\ntoSpace <- content_transformer(function(x, pattern) gsub(pattern, \" \", x))\ncorpus.preprocess <- function(corpus){\n  # Helper function to preprocess corpus\n  processed.corpus <- corpus %>%\n    tm_map(toSpace, \"/|@|\\\\|\") %>%\n    tm_map(content_transformer(tolower)) %>%\n    tm_map(removeNumbers) %>%\n    tm_map(removePunctuation) %>%\n    tm_map(removeWords, stopwords(\"english\")) %>%\n    tm_map(stripWhitespace)\n  return(processed.corpus)\n}\n\nnews.corpus <- VCorpus(VectorSource(news)) %>% corpus.preprocess()\n\nnews.dtm <- DocumentTermMatrix(news.corpus) %>% removeSparseTerms(0.99)\n\nBigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\nnews.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)) %>% removeSparseTerms(0.9999)\n\nTrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))\nnews.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)) %>% removeSparseTerms(0.9999)\n\n```\n\n\n### Descriptive Analysis\n\nThe US News dataset contains `r length(news)` records (5% of the total records). The length of records varies ranging from `r min(record.length$character)` to `r max(record.length$character)` characters, or from `r min(record.length$word)` words to `r max(record.length$word)` words. Their distributions are shown below:\n\n```{r record_length}\np1 <- ggplot(record.length, aes(x=character)) +\n  geom_bar(stat=\"bin\", binwidth=5) +\n  theme_bw() +\n  ggtitle(\"Record Length by Character\")\n\np2 <- ggplot(record.length, aes(x=word)) +\n  geom_bar(stat=\"bin\", binwidth=5) +\n  theme_bw() +\n  ggtitle(\"Record Length by Word\")\n\ngrid.arrange(p1, p2, ncol=2)\n\n```\n\nThe following plot shows the top 10 most frequent words in this dataset. It is clearly that all of the terms are fairly generic.\n\n```{r term_frequency}\n# Helper function to find the most frequent n words\nmost.freq <- function(dtm, n=10){\n  freq <- colSums(as.matrix(dtm))\n  result <- freq[order(freq, decreasing=TRUE)][1:n]\n  return(data_frame(term=names(result), count=result))\n}\n\nggplot(most.freq(news.dtm), aes(x=reorder(term, -count), y=count)) +\n  geom_bar(stat=\"identity\") +\n  theme_bw() +\n  theme(axis.title.x = element_blank()) +\n  ggtitle(\"Most frequent words in news\")\n\n```\n\nThe following plot shows the top 10 most frequent 2-gram in this dataset. Again, they are very common phrases in news articles.\n\n```{r 2gram_frequency}\nggplot(most.freq(news.dtm.2g), aes(x=reorder(term, -count), y=count)) +\n  geom_bar(stat=\"identity\") +\n  theme_bw() +\n  theme(axis.title.x = element_blank(),\n           axis.text.x  = element_text(angle=45, hjust=1)) +\n  ggtitle(\"Most frequent 2-gram in news\")\n\n```\n\nThe following plot shows the top 10 most frequent 3-gram in this dataset.\n\n```{r 3gram_frequency}\nggplot(most.freq(news.dtm.3g), aes(x=reorder(term, -count), y=count)) +\n  geom_bar(stat=\"identity\") +\n  theme_bw() +\n  theme(axis.title.x = element_blank(),\n           axis.text.x  = element_text(angle=45, hjust=1)) +\n  ggtitle(\"Most frequent 3-gram in news\")\n\n```\n\n\n### Strategies for Modeling and Prediction\n\nCurrently, the exact methodology to produce a model for predictive typing has not been finalized. Nevertheless we anticipate to use some of the following strategies:\n\n* n-gram model: as described in [Task 3], a n-gram model will be built based on the n-gram analysis described above.\n* back-off model: for n-grams not observed in the training material, use back-off model to estimate the conditional probability of a given word.\n* auto-correction: use edit-distance based matrix to achieve text auto-correction.\n* online model: track and analyze user's input and selection to improve the model accuracy.\n\nEventually, multiply predictive models will be constructed and combined in the application. The models will be ranked based on their accuracy and efficiency, and selectively dispatched in different context.\n\nWith regard to the Shiny app, our plan is to create an interface to simulate the typing experience on a cellphone with word prediction powered by our model. The feasibility of implementation in Shiny however is not clear with my limited web development experience. For example, can Shiny capture keyword input or an on-screen keyboard is required? \n\n\n### Github repository\nAll the R/RMD codes and dataset for this Capstone project can be found on Github: https://github.com/lifan0127/Capstone-Project",
    "created" : 1416801937481.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2052188675",
    "id" : "4E23FC4C",
    "lastKnownWriteTime" : 1416195797,
    "path" : "~/GitHub/DSCapstoneProject/RMD/milestone_report_v1.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}